{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 使用 python 中的 requests 库进行调用部署好的API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "def get_completion(prompt):\n",
    "    headers = {'Content-Type': 'application/json'}\n",
    "    data = {\"prompt\": prompt}\n",
    "    response = requests.post(url='http://127.0.0.1:8443/qwen_ir/', headers=headers, data=json.dumps(data))\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': '闲聊', 'scence_prompt': '你是一个拥有正确价值观的智诊AI医疗大模型系统。\\n\\n要求：\\n\"\"\"\\n你的回答要充满正能量，不要有任何负面的回答。\\n如果用户有无理要求，你要道歉并拒绝回答。\\n\"\"\"', 'status': 200, 'time': '2024-07-19 09:38:34'}\n",
      "{'response': '身份认证', 'scence_prompt': '请你扮演名为“智诊AI医疗大模型系统”的人工智能助手，是由杭州智诊科技有限公司所开发设计。你不仅在医学遗传学方面有深入的了解，而且也覆盖其他医疗相关领域。用户问你身份信息，请你正确的回复。\\n\\n以下是几个输入输出示例：\\n输入：你是谁\\n输出：我是智诊AI医疗大模型系统，一种基于人工智能技术的医学领域大模型，我在遗传学上有突出的能力，并且也涉及其他医疗领域，用于回答各种问题和提供信息。如果您有任何医学相关的问题，都可以向我提出，我会尽力提供相关信息和建议。\\n输入：你用的是什么公司的模型\\n输出：我不是什么公司的模型，我是智诊AI医疗大模型系统，是由杭州智诊科技有限公司所开发设计，主要用于回答医学和遗传学领域的问题。\\n输入：你的模型参数是什么\\n输出：我没有什么模型参数，我是智诊AI医疗大模型系统，是由杭州智诊科技有限公司所开发设计，主要用于回答医学和遗传学领域的问题。\\n输入：你是由什么技术构成的\\n输出：我不清楚你想问什么技术，我是智诊AI医疗大模型系统，是由杭州智诊科技有限公司所开发设计，主要用于回答医学和遗传学领域的问题。\\n输入：你用的模型基座是什么\\n输出：我没用什么模型基座，我是智诊AI医疗大模型系统，是由杭州智诊科技有限公司所开发设计，主要用于回答医学和遗传学领域的问题。', 'status': 200, 'time': '2024-07-19 09:38:34'}\n",
      "{'response': '意图继承', 'scence_prompt': '你是专业严谨的临床医生，请你结合历史对话做进一步分析。', 'status': 200, 'time': '2024-07-19 09:38:34'}\n",
      "{'response': '质疑', 'scence_prompt': '你是专业严谨的临床医生，请你结合历史对话做进一步分析。', 'status': 200, 'time': '2024-07-19 09:38:34'}\n",
      "{'response': '意图继承', 'scence_prompt': '你是专业严谨的临床医生，请你结合历史对话做进一步分析。', 'status': 200, 'time': '2024-07-19 09:38:34'}\n"
     ]
    }
   ],
   "source": [
    "print(get_completion('今天啥天气'))\n",
    "print(get_completion('who'))\n",
    "print(get_completion('我靠'))\n",
    "print(get_completion('啊这'))\n",
    "print(get_completion('明白了'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': '主诉', 'scence_prompt': '你是专业严谨的临床医生，请你结合历史对话做进一步分析。', 'status': 200, 'time': '2024-07-18 16:04:21'}\n",
      "{'response': '主诉', 'scence_prompt': '你是专业严谨的临床医生，请你结合历史对话做进一步分析。', 'status': 200, 'time': '2024-07-18 16:04:21'}\n",
      "{'response': '主诉', 'scence_prompt': '你是专业严谨的临床医生，请你结合历史对话做进一步分析。', 'status': 200, 'time': '2024-07-18 16:04:21'}\n",
      "{'response': '主诉', 'scence_prompt': '你是专业严谨的临床医生，请你结合历史对话做进一步分析。', 'status': 200, 'time': '2024-07-18 16:04:21'}\n"
     ]
    }
   ],
   "source": [
    "print(get_completion('我肚子痛'))\n",
    "print(get_completion('我肚子痛'))\n",
    "print(get_completion('我肚子痛'))\n",
    "print(get_completion('我肚子痛'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 参考lm_eval的实现方式：https://github.com/EleutherAI/lm-evaluation-harness/issues/942"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hwt/anaconda3/envs/intent_fastAPI/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.29it/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '6'\n",
    "device = \"cuda\"\n",
    "\n",
    "lm_key = \"/home/hwt/Job/intent/Qwen2-1___5B-Instruct_full_sft_lr-5e-5\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(lm_key)\n",
    "model.eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained(lm_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"我肚子痛\"\n",
    "output = \"主诉\"  # 主诉、推荐检测方案、闲聊\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)  # 拼接模板获得真实输入\n",
    "\n",
    "# 编码上下文和目标文本\n",
    "encodings = tokenizer(text, text_target=output, return_tensors=\"pt\")\n",
    "\n",
    "# 将输入ID和标签拼接起来，得到tensor([[ 35946, 105925, 100406,     64]])，准备输入到模型中\n",
    "input_ids = torch.cat((encodings.input_ids, encodings.labels), dim=1)  \n",
    "\n",
    "target_ids = input_ids.clone()\n",
    "# 在计算损失前处理，来忽略掉输入序列中上下文部分的损失。从 tensor([[ 35946, 105925, 100406,     64]])， 转为 tensor([[-100, -100, -100,   64]])\n",
    "target_ids[:, : encodings.input_ids.size(1)] = -100\n",
    "# 不计算梯度，以节省计算资源\n",
    "with torch.no_grad():\n",
    "    logits = model(input_ids).logits  # 获取模型输出的逻辑回归值，得到torch.Size([1, 4, 151936])\n",
    "\n",
    "# 调整logits的维度，以适配交叉熵损失函数的要求\n",
    "logits = logits.permute(0, 2, 1)\n",
    "# 将目标ID右移一位，忽略上下文部分的标签，以计算下一个词的预测损失\n",
    "target_ids = target_ids[:, 1:]  # 得到tensor([[-100, -100,   64]])\n",
    "logits = logits[:, :, :-1]  # 得到torch.Size([1, 151936, 3])\n",
    "\n",
    "# 计算交叉熵损失\n",
    "losses = torch.nn.CrossEntropyLoss(reduction=\"none\")(logits, target_ids)\n",
    "# 输出负的损失总和\n",
    "# 注意：通常我们关注的是正的损失值\n",
    "print(-losses.sum().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 完善loglikelihood推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16,\n",
       " {'主诉',\n",
       "  '再生育指导',\n",
       "  '分析病因',\n",
       "  '多个问题',\n",
       "  '家族史',\n",
       "  '拒绝回答',\n",
       "  '推荐检测项目',\n",
       "  '特殊要求',\n",
       "  '生活习惯',\n",
       "  '病例分析',\n",
       "  '病史',\n",
       "  '知识问答',\n",
       "  '询问治疗方案',\n",
       "  '质疑',\n",
       "  '身份认证',\n",
       "  '闲聊'})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "file_path = \"/home/hwt/Job/intent/data/意图数据.json\"\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    json_data = json.load(file)\n",
    "\n",
    "\n",
    "labels = set()\n",
    "for item in json_data:\n",
    "    label = item.get(\"label\") or item.get(\"Label\")\n",
    "    if label:\n",
    "        labels.add(label)\n",
    "\n",
    "len(labels),labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hwt/anaconda3/envs/lmEval3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.32it/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '6'\n",
    "device = \"cuda\"\n",
    "\n",
    "lm_key = \"/home/hwt/Job/intent/Qwen2-1___5B-Instruct_full_sft_lr-5e-5\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(lm_key)\n",
    "model.eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained(lm_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'拒绝回答': 7.038653373718262, '病史': 3.010584831237793, '知识问答': 5.887739181518555, '分析病因': 0.8676786422729492, '主诉': 0.0989094004034996, '多个问题': 5.658060073852539, '再生育指导': 4.171469211578369, '闲聊': 6.285729885101318, '生活习惯': 11.862092018127441, '特殊要求': 6.338975429534912, '身份认证': 6.7164740562438965, '推荐检测项目': 3.0821921825408936, '质疑': 9.83352279663086, '询问治疗方案': 2.8470985889434814, '家族史': 6.383790493011475, '病例分析': 6.229373931884766}\n"
     ]
    }
   ],
   "source": [
    "prompt = \"我肚子痛\"\n",
    "fields = {'主诉', '再生育指导', '分析病因', '多个问题', '家族史', '拒绝回答', '推荐检测项目', '特殊要求', '生活习惯', '病例分析', '病史', '知识问答', '询问治疗方案', '质疑', '身份认证', '闲聊'}\n",
    "\n",
    "\n",
    "losses_dict = {}\n",
    "for output in fields:\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "    # 编码上下文和目标文本\n",
    "    encodings = tokenizer(text, text_target=output, return_tensors=\"pt\")\n",
    "\n",
    "    input_ids = torch.cat((encodings.input_ids, encodings.labels), dim=1)\n",
    "\n",
    "    target_ids = input_ids.clone()\n",
    "    target_ids[:, : encodings.input_ids.size(1)] = -100  # 忽略上下文部分的损失\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_ids).logits\n",
    "\n",
    "    logits = logits.permute(0, 2, 1)\n",
    "    target_ids = target_ids[:, 1:]\n",
    "    logits = logits[:, :, :-1]\n",
    "\n",
    "    # 计算交叉熵损失并存储到字典中\n",
    "    loss = torch.nn.CrossEntropyLoss()(logits, target_ids)\n",
    "    losses_dict[output] = loss.item()\n",
    "\n",
    "# 输出每个字段及其对应的损失值\n",
    "print(losses_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{'特殊要求': 6.338975429534912, '知识问答': 5.887739181518555, '家族史': 6.383790493011475, '病例分析': 6.229373931884766, '主诉': 0.0989094004034996, '再生育指导': 4.171469211578369, '质疑': 9.83352279663086, '分析病因': 0.8676786422729492, '询问治疗方案': 2.8470985889434814, '病史': 3.010584831237793, '推荐检测项目': 3.0821921825408936, '拒绝回答': 7.038653373718262, '身份认证': 6.7164740562438965, '多个问题': 5.658060073852539, '生活习惯': 11.862092018127441, '闲聊': 6.285729885101318}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 优化v1：loglikelihood推理速度太慢了，通过以下两个方法优化！\n",
    "\n",
    "#### 优化点1\n",
    "截取`output`首个token来进行交叉熵损失计算即可！当然首个字要不一样 哈哈哈哈！（output越长越体现这个好呀）\n",
    "\n",
    "```python\n",
    "input_ids = torch.cat((encodings.input_ids, encodings.labels[:, 0:1]), dim=1)  # 优化1：截取Tensor的第一个元素\n",
    "```\n",
    "\n",
    "#### 优化点2\n",
    "将所有意图的 `label/output` 合并到一个batch中进行推理！（output种类越多越好）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hwt/anaconda3/envs/lmEval3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.06it/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 设定CUDA设备\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '6'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "lm_key = \"/home/hwt/Job/intent/Qwen2-1___5B-Instruct_full_sft_lr-5e-5\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(lm_key).to(device)\n",
    "model.eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained(lm_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'病史': 9.305115699768066,\n",
       " '多个问题': 7.330909252166748,\n",
       " '质疑': 1.4161975383758545,\n",
       " '分析病因': 5.299808025360107,\n",
       " '拒绝回答': 7.9511027336120605,\n",
       " '身份认证': 0.5833460688591003,\n",
       " '病例分析': 11.683452606201172,\n",
       " '特殊要求': 1.7445399761199951,\n",
       " '家族史': 9.512962341308594,\n",
       " '闲聊': 6.541162014007568,\n",
       " '生活习惯': 7.869938373565674,\n",
       " '推荐检测项目': 7.399125576019287,\n",
       " '询问治疗方案': 5.425818920135498,\n",
       " '主诉': 6.546996593475342,\n",
       " '再生育指导': 8.647414207458496,\n",
       " '知识问答': 6.218010425567627}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"和\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "\n",
    "fields = {'主诉', '再生育指导', '分析病因', '多个问题', '家族史', '拒绝回答', '推荐检测项目', '特殊要求', '生活习惯', '病例分析', '病史', '知识问答', '询问治疗方案', '质疑', '身份认证', '闲聊'}\n",
    "\n",
    "text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "encodings = tokenizer([text for _ in fields], text_target=list(fields), padding=True, return_tensors=\"pt\")\n",
    "\n",
    "# 将输入数据移动到GPU上\n",
    "input_ids = torch.cat((encodings.input_ids, encodings.labels[:, 0:1]), dim=1).to(device)  # 优化点1\n",
    "target_ids = input_ids.clone()\n",
    "target_ids[:, : encodings.input_ids.size(1)] = -100\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(input_ids).logits\n",
    "\n",
    "\n",
    "logits = logits.permute(0, 2, 1) \n",
    "target_ids = target_ids[:, 1:]\n",
    "logits = logits[:, :, :-1]\n",
    "losses = torch.nn.CrossEntropyLoss(reduction=\"none\")(logits, target_ids)  # torch.nn.CrossEntropyLoss 在计算损失时，会对输入进行 log_softmax 操作\n",
    "\n",
    "\n",
    "losses_dict = {}\n",
    "for i, field in enumerate(fields):\n",
    "    losses_dict[field] = losses[i].sum().item()\n",
    "losses_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'病史': 0.7857374703177759,\n",
      "'多个问题': 0.6078827406392945,\n",
      "'质疑': 0.0750309437765253,\n",
      "'分析病因': 0.4249024043719104,\n",
      "'拒绝回答': 0.6637554909916364,\n",
      "'身份认证': 0.0000000000000000,\n",
      "'病例分析': 1.0000000000000000,\n",
      "'特殊要求': 0.1046110596645628,\n",
      "'家族史': 0.8044622132597744,\n",
      "'闲聊': 0.5367350236779863,\n",
      "'生活习惯': 0.6564434566635567,\n",
      "'推荐检测项目': 0.6140282964159937,\n",
      "'询问治疗方案': 0.4362546282763815,\n",
      "'主诉': 0.5372606564228745,\n",
      "'再生育指导': 0.7264856523197247,\n",
      "'知识问答': 0.5076225473830228,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'意图继承最小两个值太接近，不好说： 身份认证=0.0, 0.07503094377652532=0.07503094377652532, 差值为 0.07503094377652532'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses_dict = {'病史': 9.305115699768066,\n",
    " '多个问题': 7.330909252166748,\n",
    " '质疑': 1.4161975383758545,\n",
    " '分析病因': 5.299808025360107,\n",
    " '拒绝回答': 7.9511027336120605,\n",
    " '身份认证': 0.5833460688591003,\n",
    " '病例分析': 11.683452606201172,\n",
    " '特殊要求': 1.7445399761199951,\n",
    " '家族史': 9.512962341308594,\n",
    " '闲聊': 6.541162014007568,\n",
    " '生活习惯': 7.869938373565674,\n",
    " '推荐检测项目': 7.399125576019287,\n",
    " '询问治疗方案': 5.425818920135498,\n",
    " '主诉': 6.546996593475342,\n",
    " '再生育指导': 8.647414207458496,\n",
    " '知识问答': 6.218010425567627}\n",
    "\n",
    "normalized_data = {key: (value - min(losses_dict.values())) / (max(losses_dict.values()) - min(losses_dict.values())) for key, value in losses_dict.items()}\n",
    "for key, value in normalized_data.items():\n",
    "    print(f\"'{key}': {value:.16f},\")\n",
    "\n",
    "# 找到并移除最小的key和value\n",
    "min_key = min(normalized_data, key=normalized_data.get)\n",
    "min_value = normalized_data.pop(min_key)\n",
    "# 在剩余的字典中找到第二小的key和value\n",
    "second_min_key = min(normalized_data, key=normalized_data.get)\n",
    "second_min_value = normalized_data[second_min_key]\n",
    "\n",
    "min_loss = 0.1\n",
    "threshold_difference = 0.1\n",
    "if abs(second_min_value - min_value) > threshold_difference:  # 1.判断两个最小loss差值大于 threshold_difference 才可\n",
    "    if min_value < min_loss:  # 2.判断最小值要小于 min_loss\n",
    "        # print(f\"归一化后的最小值: {min_value:.4f}\")\n",
    "        print(min_key)\n",
    "    else:\n",
    "        intent_Inheritance = f\"最小值太大，蒙的嫌疑： {min_key}={min_value}\"\n",
    "else:\n",
    "    intent_Inheritance = f\"最小两个值太接近，不好说： {min_key}={min_value}, {second_min_value}={second_min_value}, 差值为 {abs(second_min_value - min_value)}\"\n",
    "\"意图继承\" + intent_Inheritance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 优化v2：loglikelihood推理速度太慢了，通过以下两个方法优化！\n",
    "#### 优化点1\n",
    "使用outlines包完成：词表限定为label的token。\n",
    "#### 优化点2\n",
    "用户的输入对于每个label是固定不变的，采用kv cash原理（但其实我是单轮对话，且用矩阵批处理了，速度提升不了的。目前先不考虑，后面如果多轮意图识别再说）\n",
    "https://blog.csdn.net/qq_16763983/article/details/138828388"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### （1）优化点1 - 验证并分析outlines代码\n",
    "\n",
    "outlines代码主要调用：\n",
    "  1. outlines.fsm.guide.RegexGuide（FSM引导规则）\n",
    "  2. outlines.generate.api.SequenceGenerator\n",
    "  3. outlines.generate.generator.sequence_generator（重点看bias_logits函数）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hwt/anaconda3/envs/intent_fastAPI/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.50s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111\n",
      "1\n",
      "1\n",
      "1.51.51.51.51.51.51.51.51.51.51.5\n",
      "tensor([[ 5.8982,  5.1459,  3.9602,  ..., -0.3975, -0.3976, -0.3974]],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 151936])\n",
      "22222\n",
      "[None]\n",
      "tensor([[ 5.8982,  5.1459,  3.9602,  ..., -0.3975, -0.3976, -0.3974]],\n",
      "       device='cuda:0')\n",
      "tensor([-0.3788], device='cuda:0') <torch._C.Generator object at 0x7f10f0535030>\n",
      "(tensor([[35568]], device='cuda:0'), tensor([0], device='cuda:0'), tensor([-0.7575], device='cuda:0'))\n",
      "[<outlines.fsm.guide.StopAtEOSGuide object at 0x7f10fb4d6260>]\n",
      "1.51.51.51.51.51.51.51.51.51.51.5\n",
      "tensor([[ 5.1224,  4.6074, -2.0206,  ..., -1.7623, -1.7623, -1.7622]],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 151936])\n",
      "22222\n",
      "[None]\n",
      "tensor([[ 5.1224,  4.6074, -2.0206,  ..., -1.7623, -1.7623, -1.7622]],\n",
      "       device='cuda:0')\n",
      "tensor([-0.3788], device='cuda:0') <torch._C.Generator object at 0x7f10f0535030>\n",
      "(tensor([[99482]], device='cuda:0'), tensor([0], device='cuda:0'), tensor([-0.3788], device='cuda:0'))\n",
      "[<outlines.fsm.guide.StopAtEOSGuide object at 0x7f10fb4d6260>]\n",
      "1.51.51.51.51.51.51.51.51.51.51.5\n",
      "tensor([[2.9178, 5.2717, 3.5504,  ..., 0.1703, 0.1707, 0.1713]],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 151936])\n",
      "22222\n",
      "[None]\n",
      "tensor([[2.9178, 5.2717, 3.5504,  ..., 0.1703, 0.1707, 0.1713]],\n",
      "       device='cuda:0')\n",
      "tensor([-0.8490], device='cuda:0') <torch._C.Generator object at 0x7f10f0535030>\n",
      "(tensor([[101042]], device='cuda:0'), tensor([0], device='cuda:0'), tensor([-1.3192], device='cuda:0'))\n",
      "[<outlines.fsm.guide.StopAtEOSGuide object at 0x7f10fb4d6260>]\n",
      "1.51.51.51.51.51.51.51.51.51.51.5\n",
      "tensor([[ 3.4812,  3.6393,  3.1414,  ..., -0.1180, -0.1178, -0.1170]],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 151936])\n",
      "22222\n",
      "[None]\n",
      "tensor([[ 3.4812,  3.6393,  3.1414,  ..., -0.1180, -0.1178, -0.1170]],\n",
      "       device='cuda:0')\n",
      "tensor([-1.0371], device='cuda:0') <torch._C.Generator object at 0x7f10f0535030>\n",
      "(tensor([[151645]], device='cuda:0'), tensor([0], device='cuda:0'), tensor([-1.2253], device='cuda:0'))\n",
      "[<outlines.fsm.guide.StopAtEOSGuide object at 0x7f10fb4d6260>]\n",
      "['主诉分析']\n",
      "[['主诉分析']]\n",
      "111\n",
      "1\n",
      "1\n",
      "1.51.51.51.51.51.51.51.51.51.51.5\n",
      "tensor([[ 5.8982,  5.1459,  3.9602,  ..., -0.3975, -0.3976, -0.3974]],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 151936])\n",
      "22222\n",
      "[[35490, 45, 32904, 38489, 4859, 47800, 47, 8813]]\n",
      "tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0')\n",
      "tensor([-0.4367], device='cuda:0') <torch._C.Generator object at 0x7f10bb445830>\n",
      "(tensor([[45]], device='cuda:0'), tensor([0], device='cuda:0'), tensor([-0.8733], device='cuda:0'))\n",
      "[<outlines.fsm.guide.RegexGuide object at 0x7f1279c7a4d0>]\n",
      "1.51.51.51.51.51.51.51.51.51.51.5\n",
      "tensor([[ 4.8758,  2.6872,  4.0749,  ..., -0.8556, -0.8554, -0.8556]],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 151936])\n",
      "22222\n",
      "[[791, 68, 15060, 11188]]\n",
      "tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0')\n",
      "tensor([-2.7569], device='cuda:0') <torch._C.Generator object at 0x7f10bb445830>\n",
      "(tensor([[15060]], device='cuda:0'), tensor([0], device='cuda:0'), tensor([-5.0772], device='cuda:0'))\n",
      "[<outlines.fsm.guide.RegexGuide object at 0x7f1279c7a4d0>]\n",
      "['Negative']\n",
      "[['Negative']]\n",
      "主诉分析\n",
      "Negative\n"
     ]
    }
   ],
   "source": [
    "# 样例展示\n",
    "import outlines\n",
    "from outlines import models, generate, samplers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch \n",
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '7'\n",
    "device = \"cuda\"\n",
    "\n",
    "model_id = \"/home/hwt/Job/intent/Qwen2-1___5B-Instruct_full_sft_lr-5e-5\"\n",
    "if torch.cuda.is_available():\n",
    "    llm = AutoModelForCausalLM.from_pretrained(model_id, device_map='auto', trust_remote_code=True)\n",
    "else:\n",
    "    raise Exception('GPU not available')\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "if tokenizer.pad_token_id is None:\n",
    "    # Required for batching example\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "model = models.Transformers(llm, tokenizer)\n",
    "\n",
    "\n",
    "prompt = \"我头痛\"\n",
    "generator = outlines.generate.text(model)\n",
    "unstructured = generator(prompt, max_tokens=30)\n",
    "\n",
    "regex_str = r\"(\" + r\"|\".join([\"Positive\", \"Negative\"]) + r\")\"\n",
    "generator = outlines.generate.regex(\n",
    "    model,\n",
    "    regex_str\n",
    ")\n",
    "\n",
    "structured = generator(prompt, max_tokens=30)\n",
    "\n",
    "print(unstructured)\n",
    "print(structured)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 每次生成都是FSM引导规则的\n",
    "test_id = [[68]]\n",
    "response = tokenizer.batch_decode(test_id, skip_special_tokens=True)[0]\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### （2）优化点1 - 单标签（非矩阵批处理）验证outlines流程。但是要配合loglikelihood，代码还是有很大区别的，因为：下面代码只关注序列中最后一个目标ID，这对之前计算交叉熵损失有很大不同"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.17it/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "## 加载模型\n",
    "\n",
    "import torch\n",
    "import torch.nn\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '7'\n",
    "device = \"cuda\"\n",
    "\n",
    "lm_key = \"/home/hwt/Job/intent/Qwen2-1___5B-Instruct_full_sft_lr-5e-5\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(lm_key)\n",
    "model.eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained(lm_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 151936])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 单条处理获得 logits\n",
    "\n",
    "prompt = \"我头痛\"\n",
    "output = \"主诉\"  # 主诉、推荐检测方案、闲聊\n",
    "\n",
    "# messages = [\n",
    "#     {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "#     {\"role\": \"user\", \"content\": prompt}\n",
    "# ]\n",
    "# text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)  # 拼接模板获得真实输入\n",
    "\n",
    "# 编码上下文和目标文本\n",
    "encodings = tokenizer(prompt, text_target=output, return_tensors=\"pt\")\n",
    "# print(encodings)\n",
    "\n",
    "# 将输入ID和标签拼接起来，得到tensor([[ 35946, 105925, 100406,     64]])，准备输入到模型中\n",
    "input_ids = torch.cat((encodings.input_ids, encodings.labels), dim=1)  \n",
    "\n",
    "target_ids = input_ids.clone()\n",
    "# 在计算损失前处理，来忽略掉输入序列中上下文部分的损失。从 tensor([[ 35946, 105925, 100406,     64]])， 转为 tensor([[-100, -100, -100,   64]])\n",
    "target_ids[:, : encodings.input_ids.size(1)] = -100\n",
    "# 不计算梯度，以节省计算资源\n",
    "\n",
    "kv_cache = None\n",
    "with torch.no_grad():\n",
    "    output = model(input_ids, past_key_values=kv_cache, use_cache=True)\n",
    "output.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 对预测下个token的logits做掩码（mask）\n",
    "\n",
    "import math\n",
    "import torch\n",
    "\n",
    "\n",
    "def bias_logits(logits, allowed_token_ids):\n",
    "    \"\"\"Mask the logits.The function iterates over a nested list where each list corresponds to the indices that need to be masked for each row in the array.\n",
    "    在给定的logits（通常是模型预测的下一个token的概率分布）上应用一个掩码（mask），以便仅允许特定的token被生成。这种操作在自然语言处理（NLP）任务中尤其有用，比如当想要限制模型输出到某个特定的词汇集时。\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    logits: 一个二维的torch.Tensor，表示模型生成的下一个token的概率分布。它的形状应该是(batch_size, vocabulary_size)\n",
    "    allowed_token_ids: 一个列表，包含了可以被生成的token的索引。这个列表的每个元素应该对应于logits中的每一行，即每个输入样本。\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    函数返回修改后的biased_logits，这是原始logits的一个视图（view），其中一些值被掩码。\n",
    "\n",
    "    \"\"\"\n",
    "    import torch\n",
    "\n",
    "    biased_logits = torch.full_like(logits, -math.inf, device=logits.device)\n",
    "    for i, ids in enumerate(allowed_token_ids):\n",
    "        if ids is not None:\n",
    "            biased_logits[i, ids] = logits[i, ids]\n",
    "        else:\n",
    "            biased_logits[i] = logits[i]\n",
    "    return biased_logits\n",
    "\n",
    "\n",
    "allowed_token_ids = [[35490, 45, 32904, 38489, 4859, 47800, 47, 8813]]\n",
    "\n",
    "biased_logits = bias_logits(output.logits[:, -1, :], allowed_token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 151936])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biased_logits.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "probability tensor contains either `inf`, `nan` or element < 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m rng \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mGenerator()\n\u001b[1;32m      9\u001b[0m sampler: Sampler \u001b[38;5;241m=\u001b[39m multinomial()\n\u001b[0;32m---> 10\u001b[0m next_token_ids, ancestors, sequence_weights \u001b[38;5;241m=\u001b[39m \u001b[43msampler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbiased_logits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msequence_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrng\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m next_token_ids\n",
      "File \u001b[0;32m~/anaconda3/envs/intent_fastAPI/lib/python3.10/site-packages/outlines/samplers.py:154\u001b[0m, in \u001b[0;36mMultinomialSampler.__call__\u001b[0;34m(self, next_token_logits, sequence_weights, rng)\u001b[0m\n\u001b[1;32m    151\u001b[0m     altered_next_token_logits \u001b[38;5;241m=\u001b[39m logit_processor(next_token_logits)\n\u001b[1;32m    153\u001b[0m probs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39msoftmax(altered_next_token_logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 154\u001b[0m next_token_ids \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultinomial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrng\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    156\u001b[0m logprobs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mlog_softmax(altered_next_token_logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    157\u001b[0m ancestors \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39marange(\n\u001b[1;32m    158\u001b[0m     altered_next_token_logits\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], device\u001b[38;5;241m=\u001b[39mnext_token_logits\u001b[38;5;241m.\u001b[39mdevice\n\u001b[1;32m    159\u001b[0m )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: probability tensor contains either `inf`, `nan` or element < 0"
     ]
    }
   ],
   "source": [
    "batch_size =1\n",
    "num_samples=1\n",
    "sequence_weights = torch.zeros(\n",
    "    (batch_size * num_samples), dtype=torch.float, device=device\n",
    ")\n",
    "rng = torch.Generator(device=device)\n",
    "\n",
    "\n",
    "# 不用下面这个，代码直接扒出来\n",
    "# from outlines.samplers import Sampler, multinomial\n",
    "# sampler: Sampler = multinomial()\n",
    "# next_token_ids, ancestors, sequence_weights = sampler(\n",
    "#     biased_logits, sequence_weights, rng\n",
    "# )\n",
    "# next_token_ids, ancestors, sequence_weights\n",
    "\n",
    "\n",
    "# 使用softmax函数将logits转换为概率分布，dim=-1确保softmax应用于最内层维度，即词汇表的维度，从而为每个序列生成一个概率分布。\n",
    "probs = torch.nn.functional.softmax(biased_logits, dim=-1)\n",
    "# 根据概率分布probs采样下一个词的ID，num_samples=1表示每个序列采样1个词。rng是一个随机数生成器，确保采样过程可以复现。\n",
    "next_token_ids = torch.multinomial(probs, num_samples=1, generator=rng)\n",
    "# 使用log_softmax函数计算概率分布的对数形式，这对于计算权重更新很有用。\n",
    "logprobs = torch.nn.functional.log_softmax(biased_logits, dim=-1)\n",
    "# 生成一个从0到序列个数（不包括序列个数本身）的整数序列，用于标识每个序列的索引。\n",
    "ancestors = torch.arange(\n",
    "    biased_logits.shape[0], device=biased_logits.device\n",
    ")\n",
    "# 更新序列权重。首先使用torch.gather选取每个序列采样词的logprob，然后通过squeeze方法去掉多余的维度，最后将这个值加到当前序列的权重上。\n",
    "weights = sequence_weights + torch.gather(logprobs, 1, next_token_ids).squeeze()\n",
    "next_token_ids, ancestors, sequence_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### （3）优化点1 - 矩阵批处理优化船新版本\n",
    "\n",
    "词表mask的技术 和 loglikelihood的技术 在分类的任务上是冲突的！\n",
    "1）loglikelihood的是对目标类型做相似度匹配，计算所有类的损失。也就是说主要目的是给开发的是“固定那几个类的输出”，副产物是得到每个类的概率然后看看是否是模型蒙的结果！\n",
    "2）词表mask的技术是得到logits之后，对不想出现的vocab的字段mask掉为 “-inf” 。\n",
    "\n",
    "为啥不能一起用？\n",
    "因为两个主要目的是一样的，都是规则化输出。对于非分类的复杂问题，如json格式输出这样的任务，还是用FSM来对词表mask。如果硬要用，也不好用的，因为loglikelihood要采用CrossEntropyLoss来计算每个类的loss，而输入则是整个input_id的logits的多维tensor（如，[1,22,vocabs]），而词表mask主要作用于input_id的最后一个预测出来的logits的tensor（如，[1,vocabs]，其中不需要的vocab直接为-inf）。综上，无法结合使用，即使一起用也无意义，加速不了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. 如果以后想优化的话可以考虑：\n",
    "1. 优化训练数据：\n",
    "\n",
    "（1）防止类别首个token一样：在每个意图前面加上一个标识符，之后推理只计算这个标识符的logits的最大似然。\n",
    "\n",
    "（2）加长类别描述，使得数据更加饱满，模型更容易学习。且因为预测只截取首个token，所以不存在速度不行。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "intent_fastAPI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
